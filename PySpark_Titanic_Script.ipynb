{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with PySpark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In general, we use Spark for big data processing. We will utilize PySpark, which is the Python wrapper to connect the Python IDE to the Java engine, as Spark is written in Java. Spark SQL is a Spark module for structured data processing. It can execute SQL queries and contains a dataframe for scaling the data analysis for big data. The Spark ML library performs machine learning in Spark. It makes machine learning scalable for big data. For this project, we will use algorithms such as logistic regression (Ridge and Lasso Regressions) and Gradient Boosting. Generally, there are two types of operations in Spark, transformations and actions. Transformations creates a new dataframe from the previous one, and actions compute a result based on a dataframe and returns a value to the driver program.\n",
    "\n",
    "### Objective\n",
    "\n",
    "The objective of this project is to utilize the PySpark library to perform data processing and machine learning on the Titanic dataset. The ultimate goal is to predict whether a passenger survived or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import mean, col, regexp_extract, when, isnan, count, create_map, lit\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from itertools import chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first start a Spark Session and create a spark instance. We read in csv files like we would with Pandas. When we enable inferSchema, it will find the right schema for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session (like a container)\n",
    "spark = SparkSession.builder.appName('PySpark with ML').getOrCreate()\n",
    "train_df = spark.read.csv('train.csv', header=True, inferSchema=True)\n",
    "test_df = spark.read.csv('test.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the option to convert the dataframe to a Pandas dataframe. Limit() is a transformation, whereas toPandas() is an action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>None</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500  None        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250  None        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500  None        S  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "|summary|           Survived|            Pclass|               Age|             SibSp|              Parch|             Fare|\n",
      "+-------+-------------------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "|  count|                891|               891|               714|               891|                891|              891|\n",
      "|   mean| 0.3838383838383838| 2.308641975308642| 29.69911764705882|0.5230078563411896|0.38159371492704824| 32.2042079685746|\n",
      "| stddev|0.48659245426485753|0.8360712409770491|14.526497332334035|1.1027434322934315| 0.8060572211299488|49.69342859718089|\n",
      "|    min|                  0|                 1|              0.42|                 0|                  0|              0.0|\n",
      "|    25%|                  0|                 2|              20.0|                 0|                  0|           7.8958|\n",
      "|    50%|                  0|                 3|              28.0|                 0|                  0|          14.4542|\n",
      "|    75%|                  1|                 3|              38.0|                 1|                  0|             31.0|\n",
      "|    max|                  1|                 3|              80.0|                 8|                  6|         512.3292|\n",
      "+-------+-------------------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select('Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare').summary().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observe the summary for the integer and double columns (except PassengerId):\n",
    "* Survived seemed to have more passengers not survived the crash\n",
    "* Pclass has more passengers in the 3rd class than the other 2\n",
    "* Both the median and mean of Age are in the late 20s, and the youngest passenger was less than 1 year old, whereas the oldest passenger was 80 years old\n",
    "* SibSp seems to mostly have 0 or 1 siblings or spouses with a max of 8\n",
    "* Parch shows that most passengers came with no parents or children with the maximum of 6.\n",
    "* Fare is skewed to the right as the mean is greater than the median, and the highest fare price was 512."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('Survived').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from these counts, more passengers did not survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+\n",
      "|Survived|          avg(Age)|         avg(Fare)|\n",
      "+--------+------------------+------------------+\n",
      "|       1|28.343689655172415| 48.39540760233917|\n",
      "|       0| 30.62617924528302|22.117886885245877|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('Survived').mean('Age', 'Fare').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we breakdown the survivors by average Age and Fare, we can observe that although the average age between survived or not are about the same, it is apparent that those who paid more for their tickets were more likely to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+---+\n",
      "|Survived|  1|  2|  3|\n",
      "+--------+---+---+---+\n",
      "|       1|136| 87|119|\n",
      "|       0| 80| 97|372|\n",
      "+--------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('Survived').pivot('Pclass').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First class passengers had the most likely chance to survive compared to the other two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+\n",
      "|Survived|female|male|\n",
      "+--------+------+----+\n",
      "|       1|   233| 109|\n",
      "|       0|    81| 468|\n",
      "+--------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('Survived').pivot('Sex').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Females had a higher chance of surviving than males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+---+---+----+---+----+\n",
      "|Survived|  0|  1|  2|  3|   4|  5|   6|\n",
      "+--------+---+---+---+---+----+---+----+\n",
      "|       1|233| 65| 40|  3|null|  1|null|\n",
      "|       0|445| 53| 40|  2|   4|  4|   1|\n",
      "+--------+---+---+---+---+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('Survived').pivot('Parch').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+---+---+---+----+----+\n",
      "|Survived|  0|  1|  2|  3|  4|   5|   8|\n",
      "+--------+---+---+---+---+---+----+----+\n",
      "|       1|210|112| 13|  4|  3|null|null|\n",
      "|       0|398| 97| 15| 12| 15|   5|   7|\n",
      "+--------+---+---+---+---+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('Survived').pivot('SibSp').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those with a smaller family or without a partner were more likely to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in train_df.columns]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking for missing values, Age, Cabin, and Embarked had missing values, thus we must decide how to deal with these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|count(Cabin)|\n",
      "+------------+\n",
      "|         204|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select(count(train_df.Cabin)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Cabin has about 77% data missing, and Cabin and Pclass are similar, we can drop Cabin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop('Cabin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|       Q|   77|\n",
      "|    null|    2|\n",
      "|       C|  168|\n",
      "|       S|  644|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('Embarked').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|summary|   Fare|\n",
      "+-------+-------+\n",
      "|    50%|14.4542|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select('Fare').summary('50%').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since S is the majority of the Embarked column, we will replace the two null values with S. There are missing values in test data for Fare so we will fill in those missing values with the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.fillna({'Embarked':'S', 'Fare': 14.45})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will group similar titles together and assign the average age to the missing values for that title. We will first extract the titles using regular expression, and see the count and average age for each title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------------+\n",
      "|   Title|count(Age)|          avg(Age)|\n",
      "+--------+----------+------------------+\n",
      "|      Mr|       398|32.368090452261306|\n",
      "|    Miss|       146|21.773972602739725|\n",
      "|     Mrs|       108|35.898148148148145|\n",
      "|  Master|        36| 4.574166666666667|\n",
      "|     Rev|         6|43.166666666666664|\n",
      "|      Dr|         6|              42.0|\n",
      "|     Col|         2|              58.0|\n",
      "|    Mlle|         2|              24.0|\n",
      "|   Major|         2|              48.5|\n",
      "|     Don|         1|              40.0|\n",
      "|Countess|         1|              33.0|\n",
      "|    Lady|         1|              48.0|\n",
      "|Jonkheer|         1|              38.0|\n",
      "|     Mme|         1|              24.0|\n",
      "|    Capt|         1|              70.0|\n",
      "|      Ms|         1|              28.0|\n",
      "|     Sir|         1|              49.0|\n",
      "+--------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.withColumn('Title', regexp_extract(train_df['Name'], '([A-Za-z]+)\\.', 1))\n",
    "train_df.groupBy('Title').agg(count('Age'), mean('Age')).sort(col('count(Age)').desc()).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 3 titles are Mr, Miss, and Mrs which account for most of the titles of passengers. Master is lower in count than the top three, however, the average age is much lower which could account for a different group of passengers. Thus, we will map the other titles to the top 4 titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "| Title|          avg(Age)|\n",
      "+------+------------------+\n",
      "|  Miss|             21.86|\n",
      "|Master| 4.574166666666667|\n",
      "|    Mr| 33.02272727272727|\n",
      "|   Mrs|35.981818181818184|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "title_dictionary = {'Mr':'Mr', 'Miss':'Miss', 'Mrs':'Mrs','Master':'Master','Rev':'Mr', 'Dr':'Mr', 'Dona':'Mrs','Col':'Mr', 'Mlle':'Miss', 'Major': 'Mr', 'Don':'Mr', 'Countess':'Mrs', 'Lady': 'Mrs', 'Jonkheer':'Mr','Mme':'Miss', 'Capt':'Mr','Ms':'Miss','Sir':'Mr'}\n",
    "title_map = create_map([lit(x) for x in chain(*title_dictionary.items())])\n",
    "train_df = train_df.withColumn('Title', title_map[train_df['Title']])\n",
    "train_df.groupBy('Title').mean('Age').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function called age_imputer() which will fill in the missing values of Age for each given title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_imputer(df, title, age):\n",
    "    return df.withColumn('Age', when((df['Age'].isNull()) & (df['Title'] == title), age).otherwise(df['Age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = age_imputer(train_df, 'Mr', 33.02)\n",
    "train_df = age_imputer(train_df, 'Miss', 21.86)\n",
    "train_df = age_imputer(train_df, 'Mrs', 35.98)\n",
    "train_df = age_imputer(train_df, 'Master', 4.57)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will create a new column named FamilySize to have a count of total members of a family using columns Parch and SibSpl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.withColumn('FamilySize', train_df['Parch'] + train_df['SibSp']).drop('Parch', 'SibSp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also dropping other columns we do not need for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop('PassengerID', 'Name', 'Ticket', 'Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-------+--------+----------+\n",
      "|Survived|Pclass|   Sex| Age|   Fare|Embarked|FamilySize|\n",
      "+--------+------+------+----+-------+--------+----------+\n",
      "|       0|     3|  male|22.0|   7.25|       S|         1|\n",
      "|       1|     1|female|38.0|71.2833|       C|         1|\n",
      "|       1|     3|female|26.0|  7.925|       S|         0|\n",
      "|       1|     1|female|35.0|   53.1|       S|         1|\n",
      "|       0|     3|  male|35.0|   8.05|       S|         0|\n",
      "+--------+------+------+----+-------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+---+----+--------+----------+\n",
      "|Survived|Pclass|Sex|Age|Fare|Embarked|FamilySize|\n",
      "+--------+------+---+---+----+--------+----------+\n",
      "|       0|     0|  0|  0|   0|       0|         0|\n",
      "+--------+------+---+---+----+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in train_df.columns]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have no missing values and the columns that we need, so we will move on to modeling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first convert the 'Sex' and 'Embarked' columns from string to numeric index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-------+----------+------+-----------+\n",
      "|Survived|Pclass| Age|   Fare|FamilySize|SexNum|EmbarkedNum|\n",
      "+--------+------+----+-------+----------+------+-----------+\n",
      "|       0|     3|22.0|   7.25|         1|   0.0|        0.0|\n",
      "|       1|     1|38.0|71.2833|         1|   1.0|        1.0|\n",
      "|       1|     3|26.0|  7.925|         0|   1.0|        0.0|\n",
      "|       1|     1|35.0|   53.1|         1|   1.0|        0.0|\n",
      "|       0|     3|35.0|   8.05|         0|   0.0|        0.0|\n",
      "+--------+------+----+-------+----------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strInd = StringIndexer(inputCols=['Sex', 'Embarked'], outputCols=['SexNum', 'EmbarkedNum'])\n",
    "strInd_mod = strInd.fit(train_df)\n",
    "\n",
    "train_df_new = strInd_mod.transform(train_df).drop('Sex', 'Embarked')\n",
    "train_df_new.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use VectorAssembler because in scikit learn, it takes X and y in a separation matrix. Usually, y is a column vector and X is a matrix. But for Spark API, X and y has to be in a single matrix instead of two for the training data. It only accepts X in the prediction part. X should also be a vector in each row of the dataframe. We cannot directly feed the dataframe to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+\n",
      "|features                      |Survived|\n",
      "+------------------------------+--------+\n",
      "|[3.0,22.0,7.25,1.0,0.0,0.0]   |0       |\n",
      "|[1.0,38.0,71.2833,1.0,1.0,1.0]|1       |\n",
      "|[3.0,26.0,7.925,0.0,1.0,0.0]  |1       |\n",
      "|[1.0,35.0,53.1,1.0,1.0,0.0]   |1       |\n",
      "|[3.0,35.0,8.05,0.0,0.0,0.0]   |0       |\n",
      "+------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_assemble = VectorAssembler(inputCols=train_df_new.columns[1:], outputCol='features')\n",
    "train_df_new = vec_assemble.transform(train_df_new).select('features', 'Survived')\n",
    "train_df_new.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation first. Will use the test dataset later for prediction results\n",
    "train_df_sub, validation_df = train_df_new.randomSplit([0.8, 0.2], seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------+\n",
      "|features             |Survived|\n",
      "+---------------------+--------+\n",
      "|(6,[0,1],[1.0,33.02])|0       |\n",
      "|(6,[0,1],[1.0,33.02])|0       |\n",
      "|(6,[0,1],[1.0,38.0]) |0       |\n",
      "|(6,[0,1],[1.0,39.0]) |0       |\n",
      "|(6,[0,1],[1.0,40.0]) |0       |\n",
      "+---------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_sub.show(5, truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic modeling, we will use both ridge regression and lasso regression to predict whether a passenger will survive or not. Ridge regression will keep all of the features when predicting and reduces the magnitude of coefficients towards zero, whereas lasso regression will shrink the less important feature's coefficient to zero, performing feature selection.\n",
    "\n",
    "We will first use MulticlassClassificationEvaluator() and specify that we are looking to evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol = 'Survived', metricName = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8021390374331551"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rid_log = LogisticRegression(labelCol='Survived', maxIter=100, elasticNetParam=0,regParam=0.01)\n",
    "ridge_model = rid_log.fit(train_df_sub)\n",
    "ridge_pred = ridge_model.transform(validation_df)\n",
    "evaluator.evaluate(ridge_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074866310160428"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_log = LogisticRegression(labelCol='Survived', maxIter=100, elasticNetParam=1,regParam=0.01)\n",
    "lasso_model = lasso_log.fit(train_df_sub)\n",
    "lasso_pred = lasso_model.transform(validation_df)\n",
    "evaluator.evaluate(lasso_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "We will try to see if gradient boosting will give us better results than logistic regression. Gradient boosting is a prediction model in the form of an ensemble of weak prediction models, usually decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GBTClassifier(labelCol='Survived', maxIter=75, maxDepth=2)\n",
    "gb_model = gb.fit(train_df_sub)\n",
    "gb_pred = gb_model.transform(validation_df)\n",
    "evaluator.evaluate(gb_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, gradient boosting gave the best result, thus we will move forward with that and predict the test dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We will proceed with the same data preprocessing techniques we performed on the training dataset and run our gradient boosting model on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex| Age|SibSp|Parch| Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "|        892|     3|    Kelly, Mr. James|  male|34.5|    0|    0| 330911| 7.8292| null|       Q|\n",
      "|        893|     3|Wilkes, Mrs. Jame...|female|47.0|    1|    0| 363272|    7.0| null|       S|\n",
      "|        894|     2|Myles, Mr. Thomas...|  male|62.0|    0|    0| 240276| 9.6875| null|       Q|\n",
      "|        895|     3|    Wirz, Mr. Albert|  male|27.0|    0|    0| 315154| 8.6625| null|       S|\n",
      "|        896|     3|Hirvonen, Mrs. Al...|female|22.0|    1|    1|3101298|12.2875| null|       S|\n",
      "+-----------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|     0|   0|  0| 86|    0|    0|     0|   1|  327|       0|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in test_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.fillna({'Embarked':'S', 'Fare': 14.45})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.withColumn('Title', regexp_extract(test_df['Name'], '([A-Za-z]+)\\.', 1))\n",
    "test_df = test_df.withColumn('Title', title_map[test_df['Title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = age_imputer(test_df, 'Mr', 33.02)\n",
    "test_df = age_imputer(test_df, 'Miss', 21.86)\n",
    "test_df = age_imputer(test_df, 'Mrs', 35.98)\n",
    "test_df = age_imputer(test_df, 'Master', 4.57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.withColumn('FamilySize', test_df['Parch'] + test_df['SibSp']).drop('Parch', 'SibSp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop('Name', 'Ticket', 'Title', 'Cabin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+----+-------+--------+----------+\n",
      "|PassengerId|Pclass|   Sex| Age|   Fare|Embarked|FamilySize|\n",
      "+-----------+------+------+----+-------+--------+----------+\n",
      "|        892|     3|  male|34.5| 7.8292|       Q|         0|\n",
      "|        893|     3|female|47.0|    7.0|       S|         1|\n",
      "|        894|     2|  male|62.0| 9.6875|       Q|         0|\n",
      "|        895|     3|  male|27.0| 8.6625|       S|         0|\n",
      "|        896|     3|female|22.0|12.2875|       S|         2|\n",
      "+-----------+------+------+----+-------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---+---+----+--------+----------+\n",
      "|PassengerId|Pclass|Sex|Age|Fare|Embarked|FamilySize|\n",
      "+-----------+------+---+---+----+--------+----------+\n",
      "|          0|     0|  0|  0|   0|       0|         0|\n",
      "+-----------+------+---+---+----+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in test_df.columns]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the PassengerId column as we need to map this to our results at the end."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Pipeline\n",
    "\n",
    "We wil create a pipeline to have operations performed in a specific order, and in our case, we will have StringIndexer, VectorAssembler, and our gradient boosting model in one pipeline. We will also perform a cross-validated grid search over a parameter grid to find the best hyperparameters to create the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8451178451178452"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_gb = Pipeline(stages=[strInd, vec_assemble, gb])\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gb.maxIter,[5, 10, 15])\n",
    "             .addGrid(gb.maxDepth,[2,3,5])\n",
    "             .addGrid(gb.maxBins,[20, 30, 50])\n",
    "             .build())\n",
    "\n",
    "opt_model = CrossValidator(estimator=pipeline_gb, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "final_model = opt_model.fit(train_df)\n",
    "pred_train = final_model.transform(train_df)\n",
    "evaluator.evaluate(pred_train)\n",
    "# best parameters maxIter: 10, maxDepth: 3, maxBins: 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the in-sample accuracy at about 84.5%, we will use this model with these hyperparameters on the test dataset and map the PassengerId with prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|PassengerId|Survived|\n",
      "+-----------+--------+\n",
      "|        892|       0|\n",
      "|        893|       0|\n",
      "|        894|       0|\n",
      "|        895|       0|\n",
      "|        896|       1|\n",
      "+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test = final_model.transform(test_df)\n",
    "preds = pred_test.select('PassengerId', 'prediction')\n",
    "preds = preds.withColumn('Survived', preds['prediction'].cast('integer')).drop('prediction')\n",
    "preds.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will output the results to Pandas format and then to csv format. The file will be named results.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.toPandas().to_csv('results.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read in the file using the read_csv function from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('results.csv').head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "PySpark is a great tool to perform data preprocessing and machine learning, especially when it comes to big data. In this case, we were able to create a reliable model to predict if a passenger will survive or not based on the features we have. When we work with big data in the future, it would be best to utilize the PySpark library to have the best performance with data processing and data modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
